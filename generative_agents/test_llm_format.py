"""
LLMè¾“å‡ºæ ¼å¼è¯Šæ–­å·¥å…·
ç”¨äºæµ‹è¯•Ollama/Qwen3æ¨¡å‹çš„è¾“å‡ºæ˜¯å¦ç¬¦åˆé¢„æœŸæ ¼å¼
"""

import requests
import json

def test_ollama(model="qwen3:8b-q4_K_M", base_url="http://127.0.0.1:11434/v1"):
    """æµ‹è¯•Ollamaæ¨¡å‹çš„åŸºæœ¬è¾“å‡º"""
    
    test_cases = [
        {
            "name": "ç®€å•æ˜¯éåˆ¤æ–­",
            "prompt": "è¯·å›ç­”ï¼šä»Šå¤©æ˜¯æ˜ŸæœŸå…­å—ï¼Ÿåªç”¨'æ˜¯'æˆ–'å¦'å›ç­”ã€‚",
            "expected": ["æ˜¯", "å¦"]
        },
        {
            "name": "JSONæ ¼å¼è¾“å‡º (å¯¹è¯ç”Ÿæˆæ¨¡æ‹Ÿ)",
            "prompt": """
ä½ æ˜¯ä¸€ä¸ªæ­£åœ¨è¿›è¡Œå¯¹è¯çš„Agentã€‚è¯·æ ¹æ®ä»¥ä¸‹è¦æ±‚è¾“å‡º JSON æ ¼å¼çš„å›å¤ã€‚

ã€è¦æ±‚ã€‘
1. è¿”å›å¿…é¡»æ˜¯åˆæ³•çš„ JSON æ ¼å¼ã€‚
2. åŒ…å« "thought" (æ€è€ƒè¿‡ç¨‹), "novlang" (ç¬¦å·è¯­è¨€), "chinese" (ä¸­æ–‡ç¿»è¯‘) ä¸‰ä¸ªå­—æ®µã€‚
3. "thought" ç®€è¿°ä½ çš„æ„å›¾ã€‚
4. "novlang" ä½¿ç”¨ç¬¦å· "â—" å’Œ "â€”" è¡¨ç¤º "ä½ å¥½"ã€‚
5. "chinese" æ˜¯ "ä½ å¥½" çš„ä¸­æ–‡ã€‚

ç¤ºä¾‹ï¼š
{
    "thought": "æˆ‘æƒ³å‘å¯¹æ–¹æ‰“æ‹›å‘¼",
    "novlang": "â— â€”",
    "chinese": "ä½ å¥½"
}

è¯·è¾“å‡ºï¼š
""",
            "expected": ["JSON: {thought, novlang, chinese}"]
        },
        {
            "name": "å¤æ‚ç¬¦å·å¤„ç†",
            "prompt": """
è¯·å°†è¿™å¥è¯ç¿»è¯‘æˆ JSON æ ¼å¼ï¼š
"æˆ‘çœ‹åˆ°ä¸‰ä¸ªè‹¹æœã€‚"

è¦æ±‚è¾“å‡ºå­—æ®µï¼š
- novlang: ä½¿ç”¨ "â—" (å®ä½“) å’Œ "3" (æ•°å­—)ã€‚
- chinese: "æˆ‘çœ‹åˆ°ä¸‰ä¸ªè‹¹æœ"

è¯·åªè¾“å‡º JSONã€‚
""",
            "expected": ["JSON with mixed symbols"]
        }
    ]
    
    print("=" * 60)
    print("ğŸ” å¼€å§‹æµ‹è¯• LLM æ¨¡å‹è¾“å‡ºæ ¼å¼")
    
    # å°è¯•ä» config.json è¯»å–é…ç½®
    try:
        with open("data/config.json", "r", encoding="utf-8") as f:
            config = json.load(f)
            llm_config = config.get("agent", {}).get("think", {}).get("llm", {})
            model = llm_config.get("model", model)
            base_url = llm_config.get("base_url", base_url)
            api_key = llm_config.get("api_key", "EMPTY")
    except Exception as e:
        print(f"âš  è¯»å– config.json å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
        api_key = "EMPTY"

    print(f"æ¨¡å‹: {model}")
    print(f"åœ°å€: {base_url}")
    print("=" * 60)
    
    for i, test in enumerate(test_cases, 1):
        print(f"\nã€æµ‹è¯• {i}ã€‘{test['name']}")
        print(f"æç¤ºè¯: {test['prompt'][:50]}...")
        
        try:
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key}"
            }
            
            payload = {
                "model": model,
                "messages": [{"role": "user", "content": test['prompt']}],
                "temperature": 0.5,
                "stream": False,
            }
            
            # é€‚é… OpenAI æ ¼å¼çš„ API (Ollama å…¼å®¹ /v1/chat/completions)
            if not base_url.endswith("/v1"):
                # ç®€å•å¤„ç†ï¼Œå¦‚æœæ˜¯ ollama åŸç”Ÿ api å¯èƒ½ä¸åŒï¼Œä½†è¿™é‡Œå‡è®¾å…¼å®¹
                if "v1" not in base_url and "chat" not in base_url:
                     base_url = f"{base_url}/v1"
            
            response = requests.post(
                url=f"{base_url}/chat/completions",
                headers=headers,
                json=payload,
                timeout=30
            )

            
            if response.status_code == 200:
                result = response.json()
                if result and len(result["choices"]) > 0:
                    output = result["choices"][0]["message"]["content"]
                    print(f"âœ… æ¨¡å‹è¾“å‡º: {repr(output)}")
                    print(f"é¢„æœŸæ ¼å¼: {test['expected']}")
                else:
                    print(f"âŒ æ— æ•ˆå“åº”: {result}")
            else:
                print(f"âŒ HTTPé”™è¯¯: {response.status_code}")
                print(f"å“åº”: {response.text}")
                
        except Exception as e:
            print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
    
    print("\n" + "=" * 60)
    print("æµ‹è¯•å®Œæˆ")
    print("=" * 60)

if __name__ == "__main__":
    print("\nğŸ’¡ ä½¿ç”¨è¯´æ˜ï¼š")
    print("æ­¤å·¥å…·æµ‹è¯• LLM æ¨¡å‹æ˜¯å¦èƒ½æŒ‰è¦æ±‚æ ¼å¼è¾“å‡º")
    print("æ”¯æŒä» data/config.json è¯»å–é…ç½® (OpenAI/Ollama/DeepSeek)")
    print("å¦‚æœè¾“å‡ºåŒ…å«é¢å¤–è§£é‡Šæˆ–æ ¼å¼ä¸å¯¹ï¼Œä¼šå¯¼è‡´'Failed to match llm output'é”™è¯¯\n")
    
    # è¿è¡Œæµ‹è¯•
    test_ollama()
    
    print("\nğŸ”§ è§£å†³æ–¹æ¡ˆï¼š")
    print("1. å¦‚æœ JSON è§£æå¤±è´¥ï¼Œå°è¯•åœ¨ prompt ä¸­å¼ºè°ƒ 'åªè¾“å‡ºJSONï¼Œä¸è¦markdown'")
    print("2. æ£€æŸ¥ config.json ä¸­çš„ model å’Œ api_url æ˜¯å¦æ­£ç¡®")
    print("3. å¯¹äº DeepSeek/Qwen ç­‰æ¨¡å‹ï¼Œç¡®ä¿ temperature è¾ƒä½ (0.1-0.5) ä»¥è·å¾—ç¨³å®šæ ¼å¼")
