"""
è¯­è¨€æ¶Œç°åˆ†æå·¥å…·
ç”¨äºåˆ†æå››äººå¯¹è¯å®éªŒä¸­çš„Novlangç¬¦å·ä½¿ç”¨æ¨¡å¼
"""
import json
import re
from collections import Counter, defaultdict
from pathlib import Path
import argparse

# NovlangåŸºç¡€ç¬¦å·
NOVLANG_SYMBOLS = {
    'â—': 'å­˜åœ¨/å®ä½“',
    'â€”': 'çº¿æ€§/è¿‡ç¨‹',
    'ï½œ': 'å‚ç›´/ç•Œé™',
    'â–³': 'é›†åˆ/ç¾¤ä½“',
    'â–¡': 'å®¹å™¨/èŒƒå›´',
    'âˆŸ': 'å˜åŒ–/è½¬æŠ˜',
    'âŒ’': 'å…³è”/è¿æ¥',
    'âœ§': 'ç‰¹æ€§/å±æ€§',
    'â†”': 'äº’åŠ¨/äº¤æ¢',
    'âŠƒ': 'åŒ…å«/å½’å±',
    'â‰ ': 'å·®å¼‚/å¦å®š',
    'â‰ˆ': 'ç›¸ä¼¼/è¿‘ä¼¼',
    'â—‹': 'è‡ªç„¶/æœ‰æœº',
    'â•¬': 'äººå·¥/åˆ¶é€ ',
    'â™¡': 'æ„Ÿå®˜/æƒ…ç»ª',
    'âˆ': 'æ•°é‡/åº¦é‡',
    'âŠ•': 'èƒ½é‡/æ´»åŠ›',
    'âˆ…': 'ç©ºæ— /ç¼ºå¤±',
    'âˆ': 'æ°¸æ’/æŒç»­',
    'âˆ‡': 'å±‚çº§/ç§©åº',
}

NOVLANG_OPERATORS = {
    'âŠ‚': 'åµŒå¥—/é™å®š',
    'âŠ—': 'èåˆ/æ„è¯',
    'â€–': 'è°“è¯-è®ºå…ƒè¿æ¥',
    'Â·': 'æ§½åˆ†éš”ç¬¦',
    'ï¼š': 'è§£é‡Š/é”šç‚¹',
    'â†’': 'å› æœ/æ‰¿æ¥',
}

PRAGMATIC_SYMBOLS = {
    'ï¼Ÿ': 'æé—®',
    'ï¼': 'è¯·æ±‚/å‘½ä»¤',
    'âœ“': 'æ¥å—/ç¡®è®¤',
    'âœ—': 'æ‹’ç»/å¦å†³',
}


class LanguageEmergenceAnalyzer:
    def __init__(self, rounds_file):
        self.rounds_file = Path(rounds_file)
        self.data = self._load_data()
        self.symbol_usage = Counter()
        self.operator_usage = Counter()
        self.symbol_combinations = Counter()
        self.speaker_stats = defaultdict(lambda: {
            'total_messages': 0,
            'symbol_usage': Counter(),
            'avg_symbols_per_message': 0
        })
        
    def _load_data(self):
        """åŠ è½½å¯¹è¯æ•°æ®"""
        with open(self.rounds_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def extract_symbols(self, text):
        """ä»æ–‡æœ¬ä¸­æå–Novlangç¬¦å·"""
        symbols = []
        for symbol in NOVLANG_SYMBOLS.keys():
            count = text.count(symbol)
            if count > 0:
                symbols.extend([symbol] * count)
                self.symbol_usage[symbol] += count
        
        for operator in NOVLANG_OPERATORS.keys():
            count = text.count(operator)
            if count > 0:
                self.operator_usage[operator] += count
        
        return symbols
    
    def extract_combinations(self, text):
        """æå–ç¬¦å·ç»„åˆæ¨¡å¼ï¼ˆ2-3ä¸ªè¿ç»­ç¬¦å·ï¼‰"""
        # æå–æ‰€æœ‰ç¬¦å·åºåˆ—
        all_symbols = ''.join(NOVLANG_SYMBOLS.keys()) + ''.join(NOVLANG_OPERATORS.keys())
        pattern = f'[{re.escape(all_symbols)}]{{2,3}}'
        combinations = re.findall(pattern, text)
        return combinations
    
    def analyze(self):
        """æ‰§è¡Œå®Œæ•´åˆ†æ"""
        print("=" * 70)
        print("è¯­è¨€æ¶Œç°åˆ†ææŠ¥å‘Š")
        print("=" * 70)
        
        total_rounds = len(self.data)
        total_conversations = sum(len(r.get('conversations', [])) for r in self.data)
        
        print(f"\nğŸ“Š åŸºç¡€ç»Ÿè®¡")
        print(f"  æ€»è½®æ¬¡: {total_rounds}")
        print(f"  æ€»å¯¹è¯æ•°: {total_conversations}")
        
        # åˆ†ææ¯è½®å¯¹è¯
        for round_data in self.data:
            for conv in round_data.get('conversations', []):
                speaker = conv.get('speaker', '')
                content = conv.get('content', '')
                
                # æå–ç¬¦å·
                symbols = self.extract_symbols(content)
                
                # æå–ç»„åˆ
                combinations = self.extract_combinations(content)
                for combo in combinations:
                    self.symbol_combinations[combo] += 1
                
                # æ›´æ–°è¯´è¯è€…ç»Ÿè®¡
                self.speaker_stats[speaker]['total_messages'] += 1
                for symbol in symbols:
                    self.speaker_stats[speaker]['symbol_usage'][symbol] += 1
        
        # è®¡ç®—å¹³å‡å€¼
        for speaker, stats in self.speaker_stats.items():
            if stats['total_messages'] > 0:
                total_symbols = sum(stats['symbol_usage'].values())
                stats['avg_symbols_per_message'] = total_symbols / stats['total_messages']
        
        self._print_symbol_usage()
        self._print_operator_usage()
        self._print_combinations()
        self._print_speaker_stats()
        self._print_emergence_indicators()
    
    def _print_symbol_usage(self):
        """æ‰“å°ç¬¦å·ä½¿ç”¨é¢‘ç‡"""
        print(f"\nğŸ”¤ åŸºç¡€ç¬¦å·ä½¿ç”¨é¢‘ç‡ (Top 10)")
        print("-" * 70)
        for symbol, count in self.symbol_usage.most_common(10):
            meaning = NOVLANG_SYMBOLS.get(symbol, 'æœªçŸ¥')
            percentage = (count / sum(self.symbol_usage.values())) * 100
            print(f"  {symbol}  {meaning:15s}  ä½¿ç”¨æ¬¡æ•°: {count:4d}  å æ¯”: {percentage:5.2f}%")
    
    def _print_operator_usage(self):
        """æ‰“å°æ“ä½œç¬¦ä½¿ç”¨é¢‘ç‡"""
        print(f"\nğŸ”§ ç»„åˆæ ‡è®°ä½¿ç”¨é¢‘ç‡")
        print("-" * 70)
        for operator, count in self.operator_usage.most_common():
            meaning = NOVLANG_OPERATORS.get(operator, 'æœªçŸ¥')
            percentage = (count / sum(self.operator_usage.values())) * 100 if self.operator_usage else 0
            print(f"  {operator}  {meaning:20s}  ä½¿ç”¨æ¬¡æ•°: {count:4d}  å æ¯”: {percentage:5.2f}%")
    
    def _print_combinations(self):
        """æ‰“å°å¸¸è§ç¬¦å·ç»„åˆ"""
        print(f"\nğŸ”— é«˜é¢‘ç¬¦å·ç»„åˆ (Top 15)")
        print("-" * 70)
        for combo, count in self.symbol_combinations.most_common(15):
            if count >= 2:  # åªæ˜¾ç¤ºå‡ºç°2æ¬¡ä»¥ä¸Šçš„ç»„åˆ
                print(f"  {combo:10s}  å‡ºç°æ¬¡æ•°: {count:3d}")
    
    def _print_speaker_stats(self):
        """æ‰“å°å„è¯´è¯è€…çš„ç»Ÿè®¡"""
        print(f"\nğŸ‘¥ å„è§’è‰²ç»Ÿè®¡")
        print("-" * 70)
        for speaker, stats in sorted(self.speaker_stats.items()):
            print(f"\n  {speaker}")
            print(f"    æ€»å‘è¨€æ•°: {stats['total_messages']}")
            print(f"    å¹³å‡ç¬¦å·æ•°/æ¶ˆæ¯: {stats['avg_symbols_per_message']:.2f}")
            print(f"    æœ€å¸¸ç”¨ç¬¦å·:")
            for symbol, count in stats['symbol_usage'].most_common(5):
                meaning = NOVLANG_SYMBOLS.get(symbol, 'æœªçŸ¥')
                print(f"      {symbol} ({meaning}): {count}æ¬¡")
    
    def _print_emergence_indicators(self):
        """æ‰“å°è¯­è¨€æ¶Œç°æŒ‡æ ‡"""
        print(f"\nâœ¨ è¯­è¨€æ¶Œç°æŒ‡æ ‡")
        print("-" * 70)
        
        # 1. ç¬¦å·å¤šæ ·æ€§
        total_unique_symbols = len(self.symbol_usage)
        total_possible_symbols = len(NOVLANG_SYMBOLS)
        symbol_diversity = total_unique_symbols / total_possible_symbols
        print(f"  ç¬¦å·å¤šæ ·æ€§: {symbol_diversity:.2%} ({total_unique_symbols}/{total_possible_symbols})")
        
        # 2. ç»„åˆåˆ›æ–°åº¦
        unique_combinations = len(self.symbol_combinations)
        print(f"  ç‹¬ç‰¹ç»„åˆæ•°: {unique_combinations}")
        
        # 3. ç¨³å®šç»„åˆï¼ˆå‡ºç°5æ¬¡ä»¥ä¸Šï¼‰
        stable_combos = [combo for combo, count in self.symbol_combinations.items() if count >= 5]
        print(f"  ç¨³å®šç»„åˆæ•°: {len(stable_combos)} (å‡ºç°â‰¥5æ¬¡)")
        if stable_combos:
            print(f"    {', '.join(stable_combos[:10])}")
        
        # 4. ä½¿ç”¨é›†ä¸­åº¦ï¼ˆåŸºå°¼ç³»æ•°çš„ç®€åŒ–ç‰ˆï¼‰
        if self.symbol_usage:
            total_usage = sum(self.symbol_usage.values())
            top_5_usage = sum(count for _, count in self.symbol_usage.most_common(5))
            concentration = top_5_usage / total_usage
            print(f"  ç¬¦å·é›†ä¸­åº¦: {concentration:.2%} (Top5ç¬¦å·å æ¯”)")
        
        # 5. æ¼”åŒ–è¶‹åŠ¿
        if len(self.data) >= 10:
            early_rounds = self.data[:len(self.data)//2]
            late_rounds = self.data[len(self.data)//2:]
            
            early_symbols = set()
            late_symbols = set()
            
            for round_data in early_rounds:
                for conv in round_data.get('conversations', []):
                    early_symbols.update(self.extract_symbols(conv.get('content', '')))
            
            for round_data in late_rounds:
                for conv in round_data.get('conversations', []):
                    late_symbols.update(self.extract_symbols(conv.get('content', '')))
            
            new_symbols = late_symbols - early_symbols
            print(f"  æ–°å‡ºç°ç¬¦å·æ•°: {len(new_symbols)} (ååŠæœŸ)")
            if new_symbols:
                print(f"    {', '.join(sorted(new_symbols)[:10])}")
    
    def export_timeline(self, output_file):
        """å¯¼å‡ºæ—¶é—´çº¿æ•°æ®ï¼ˆå¯ç”¨äºå¯è§†åŒ–ï¼‰"""
        timeline = []
        for round_data in self.data:
            round_num = round_data.get('round', 0)
            round_symbols = Counter()
            
            for conv in round_data.get('conversations', []):
                symbols = self.extract_symbols(conv.get('content', ''))
                round_symbols.update(symbols)
            
            timeline.append({
                'round': round_num,
                'total_symbols': sum(round_symbols.values()),
                'unique_symbols': len(round_symbols),
                'top_symbols': dict(round_symbols.most_common(5))
            })
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(timeline, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ“ æ—¶é—´çº¿æ•°æ®å·²å¯¼å‡ºåˆ°: {output_file}")


def main():
    parser = argparse.ArgumentParser(description='åˆ†æè¯­è¨€æ¶Œç°å®éªŒæ•°æ®')
    parser.add_argument('rounds_file', help='rounds.jsonæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--export-timeline', help='å¯¼å‡ºæ—¶é—´çº¿æ•°æ®åˆ°æŒ‡å®šæ–‡ä»¶')
    args = parser.parse_args()
    
    analyzer = LanguageEmergenceAnalyzer(args.rounds_file)
    analyzer.analyze()
    
    if args.export_timeline:
        analyzer.export_timeline(args.export_timeline)


if __name__ == '__main__':
    main()
