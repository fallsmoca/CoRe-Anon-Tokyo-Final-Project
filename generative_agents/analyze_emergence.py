"""
è¯­è¨€æ¶Œç°åˆ†æå·¥å…·
ç”¨äºåˆ†æå››äººå¯¹è¯å®éªŒä¸­çš„Novlangç¬¦å·ä½¿ç”¨æ¨¡å¼
"""
import json
import re
from collections import Counter, defaultdict
from pathlib import Path
import argparse
import matplotlib.pyplot as plt
import numpy as np
from datetime import datetime

# NovlangåŸºç¡€ç¬¦å·
NOVLANG_SYMBOLS = {
    'â—': 'å­˜åœ¨/å®ä½“',
    'â€”': 'çº¿æ€§/è¿‡ç¨‹',
    'ï½œ': 'å‚ç›´/ç•Œé™',
    'â–³': 'é›†åˆ/ç¾¤ä½“',
    'â–¡': 'å®¹å™¨/èŒƒå›´',
    'âˆŸ': 'å˜åŒ–/è½¬æŠ˜',
    'âŒ’': 'å…³è”/è¿æ¥',
    'âœ§': 'ç‰¹æ€§/å±æ€§',
    'â†”': 'äº’åŠ¨/äº¤æ¢',
    'âŠƒ': 'åŒ…å«/å½’å±',
    'â‰ ': 'å·®å¼‚/å¦å®š',
    'â‰ˆ': 'ç›¸ä¼¼/è¿‘ä¼¼',
    'â—‹': 'è‡ªç„¶/æœ‰æœº',
    'â•¬': 'äººå·¥/åˆ¶é€ ',
    'â™¡': 'æ„Ÿå®˜/æƒ…ç»ª',
    'âˆ': 'æ•°é‡/åº¦é‡',
    'âŠ•': 'èƒ½é‡/æ´»åŠ›',
    'âˆ…': 'ç©ºæ— /ç¼ºå¤±',
    'âˆ': 'æ°¸æ’/æŒç»­',
    'âˆ‡': 'å±‚çº§/ç§©åº',
}

NOVLANG_OPERATORS = {
    'âŠ‚': 'åµŒå¥—/é™å®š',
    'âŠ—': 'èåˆ/æ„è¯',
    'â€–': 'è°“è¯-è®ºå…ƒè¿æ¥',
    'Â·': 'æ§½åˆ†éš”ç¬¦',
    'ï¼š': 'è§£é‡Š/é”šç‚¹',
    'â†’': 'å› æœ/æ‰¿æ¥',
}

PRAGMATIC_SYMBOLS = {
    'ï¼Ÿ': 'æé—®',
    'ï¼': 'è¯·æ±‚/å‘½ä»¤',
    'âœ“': 'æ¥å—/ç¡®è®¤',
    'âœ—': 'æ‹’ç»/å¦å†³',
}

# æ–°å¢ç¬¦å·ï¼ˆæ ¹æ®ä¹‹å‰çš„promptå®šä¹‰ï¼‰
NEW_SYMBOLS = {
    'â—€': 'è¿‡å»/å›æº¯',
    'â–¶': 'æœªæ¥/é¢„æœŸ',
    'â—†': 'æ ¸å¿ƒ/æœ¬è´¨',
    'ï½': 'æ¨¡ç³Š/å¯èƒ½',
}

NEW_OPERATORS = {
    'â‡”': 'ç›¸äº’å…³è”/ç­‰ä»·'
}

class LanguageEmergenceAnalyzer:
    def __init__(self, rounds_file):
        self.rounds_file = Path(rounds_file)
        self.data = self._load_data()
        self.symbol_usage = Counter()
        self.new_symbol_usage = Counter()  # æ–°å¢ï¼šä¸“é—¨ç»Ÿè®¡æ–°ç¬¦å·
        self.operator_usage = Counter()
        self.new_operator_usage = Counter()  # æ–°å¢ï¼šä¸“é—¨ç»Ÿè®¡æ–°æ“ä½œç¬¦
        self.symbol_combinations = Counter()
        self.speaker_stats = defaultdict(lambda: {
            'total_messages': 0,
            'symbol_usage': Counter(),
            'new_symbol_usage': Counter(),  # æ–°å¢ï¼šç»Ÿè®¡æ¯ä½è¯´è¯è€…çš„æ–°ç¬¦å·ä½¿ç”¨
            'avg_symbols_per_message': 0,
            'new_symbol_adoption_rate': 0  # æ–°å¢ï¼šæ–°ç¬¦å·é‡‡ç”¨ç‡
        })
        
    def _load_data(self):
        """åŠ è½½å¯¹è¯æ•°æ®"""
        with open(self.rounds_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def extract_symbols(self, text):
        """ä»æ–‡æœ¬ä¸­æå–Novlangç¬¦å·"""
        if not text:
            return []
        
        symbols = []
        # æå–åŸºç¡€ç¬¦å·
        for symbol in NOVLANG_SYMBOLS.keys():
            count = text.count(symbol)
            if count > 0:
                symbols.extend([symbol] * count)
                self.symbol_usage[symbol] += count
        
        # æå–æ–°å¢ç¬¦å·
        for symbol in NEW_SYMBOLS.keys():
            count = text.count(symbol)
            if count > 0:
                symbols.extend([symbol] * count)
                self.new_symbol_usage[symbol] += count
        
        # æå–åŸºç¡€æ“ä½œç¬¦
        for operator in NOVLANG_OPERATORS.keys():
            count = text.count(operator)
            if count > 0:
                self.operator_usage[operator] += count
        
        # æå–æ–°å¢æ“ä½œç¬¦
        for operator in NEW_OPERATORS.keys():
            count = text.count(operator)
            if count > 0:
                self.new_operator_usage[operator] += count
        
        return symbols
    
    def extract_combinations(self, text):
        """æå–ç¬¦å·ç»„åˆæ¨¡å¼ï¼ˆ2-3ä¸ªè¿ç»­ç¬¦å·ï¼‰"""
        if not text:
            return []
            
        # æå–æ‰€æœ‰ç¬¦å·åºåˆ—ï¼ˆåŒ…æ‹¬æ–°æ—§ç¬¦å·å’Œæ“ä½œç¬¦ï¼‰
        all_symbols = (''.join(NOVLANG_SYMBOLS.keys()) + 
                      ''.join(NOVLANG_OPERATORS.keys()) +
                      ''.join(NEW_SYMBOLS.keys()) +
                      ''.join(NEW_OPERATORS.keys()))
        
        # è¿™ä¸ªpatternåŒ¹é…è¿ç»­2-3ä¸ªç¬¦å·
        pattern = f'[{re.escape(all_symbols)}]{{2,3}}'
        combinations = re.findall(pattern, text)
        return combinations
    
    def trace_emergence(self, target, output_file=None):
        """è¿½è¸ªç‰¹å®šè¯æ±‡çš„æ¶Œç°è¿‡ç¨‹ (æŒ‰æ—¶é—´é¡ºåº)"""
        lines = []
        def log(s=""):
            print(s)
            lines.append(s)

        log("=" * 70)
        log(f"æ¶Œç°è¿½è¸ªæŠ¥å‘Š: '{target}'")
        log("=" * 70)
        
        found_count = 0
        
        for round_data in self.data:
            round_num = round_data.get('round', '?')
            conversations = round_data.get('conversations', [])
            
            for conv in conversations:
                novlang = conv.get('novlang', '')
                chinese = conv.get('chinese', '')
                speaker = conv.get('speaker', '')
                turn = conv.get('turn', '?')
                
                # Check if target is in novlang
                if target in novlang:
                    found_count += 1
                    log(f"\n[Round {round_num} | Turn {turn}] {speaker}:")
                    log(f"  Novlang: {novlang}")
                    log(f"  Chinese: {chinese}")  # Keep chinese for context
                    
                    # Highlight which part of novlang matched (optional but helpful if multiple matches)
                    # For now just confirming it is in novlang
                    # log(f"  > åŒ¹é…: Novlang")
        
        if found_count == 0:
            log(f"\nâš  æœªæ‰¾åˆ° '{target}' çš„ä»»ä½•è®°å½•ã€‚")
        else:
            log(f"\nâœ“ å…±æ‰¾åˆ° {found_count} æ¬¡å‡ºç°ã€‚")

        if output_file:
            try:
                with open(output_file, 'w', encoding='utf-8') as f:
                    f.write('\n'.join(lines))
                print(f"\nğŸ“ æŠ¥å‘Šå·²ä¿å­˜è‡³: {output_file}")
            except Exception as e:
                print(f"\nâŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
    
    def visualize_frequency_trends(self, output_dir=None):
        """å¯è§†åŒ–è¯é¢‘ã€è¯æ±‡ç»„å’Œæ–°è¯éšæ—¶é—´çš„å˜åŒ–è¶‹åŠ¿"""
        if output_dir is None:
            output_dir = self.rounds_file.parent
        
        # å‡†å¤‡æ•°æ®ï¼šæ¯è½®çš„è¯é¢‘ã€è¯æ±‡ç»„å’Œæ–°è¯ä½¿ç”¨æƒ…å†µ
        rounds = []
        symbol_freq_by_round = defaultdict(lambda: defaultdict(int))
        combination_freq_by_round = defaultdict(lambda: defaultdict(int))
        new_symbol_freq_by_round = defaultdict(lambda: defaultdict(int))
        
        for round_idx, round_data in enumerate(self.data):
            round_num = round_data.get('round', round_idx + 1)
            rounds.append(round_num)
            
            conversations = round_data.get('conversations', [])
            for conv in conversations:
                novlang_content = conv.get('novlang', '')
                
                # ç»Ÿè®¡ç¬¦å·ä½¿ç”¨
                for symbol in self.extract_symbols(novlang_content):
                    symbol_freq_by_round[round_num][symbol] += 1
                    
                    # å¦‚æœæ˜¯æ–°ç¬¦å·ï¼Œå•ç‹¬ç»Ÿè®¡
                    if symbol in NEW_SYMBOLS or symbol in NEW_OPERATORS:
                        new_symbol_freq_by_round[round_num][symbol] += 1
                
                # ç»Ÿè®¡ç»„åˆä½¿ç”¨
                for combo in self.extract_combinations(novlang_content):
                    combination_freq_by_round[round_num][combo] += 1
        
        # è·å–å…¨å±€å‰äº”çš„ç¬¦å·ã€ç»„åˆå’Œæ–°è¯
        all_symbols_combined = Counter()
        for round_freq in symbol_freq_by_round.values():
            for symbol, count in round_freq.items():
                all_symbols_combined[symbol] += count
        
        all_combinations_combined = Counter()
        for round_freq in combination_freq_by_round.values():
            for combo, count in round_freq.items():
                all_combinations_combined[combo] += count
        
        all_new_symbols_combined = Counter()
        for round_freq in new_symbol_freq_by_round.values():
            for symbol, count in round_freq.items():
                all_new_symbols_combined[symbol] += count
        
        # å–å‰äº”
        top_symbols = [item[0] for item in all_symbols_combined.most_common(5)]
        top_combinations = [item[0] for item in all_combinations_combined.most_common(5)]
        top_new_symbols = [item[0] for item in all_new_symbols_combined.most_common(5)]
        
        # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„æ–°è¯ï¼Œä½¿ç”¨æ‰€æœ‰æ–°è¯
        if len(top_new_symbols) < 5 and (NEW_SYMBOLS or NEW_OPERATORS):
            all_new_symbols = list(NEW_SYMBOLS.keys()) + list(NEW_OPERATORS.keys())
            top_new_symbols = all_new_symbols[:5]
        
        # åˆ›å»ºå¯è§†åŒ–
        fig, axes = plt.subplots(3, 1, figsize=(12, 15))
        
        # 1. ç¬¦å·é¢‘ç‡è¶‹åŠ¿
        ax1 = axes[0]
        for symbol in top_symbols:
            freqs = [symbol_freq_by_round[round_num].get(symbol, 0) for round_num in rounds]
            ax1.plot(rounds, freqs, marker='o', label=f'{symbol}')
        
        ax1.set_title('Top 5 Symbols Frequency Over Time', fontsize=14, fontweight='bold')
        ax1.set_xlabel('Round Number')
        ax1.set_ylabel('Frequency')
        ax1.legend(title='Symbols')
        ax1.grid(True, alpha=0.3)
        
        # 2. ç»„åˆé¢‘ç‡è¶‹åŠ¿
        ax2 = axes[1]
        for combo in top_combinations:
            freqs = [combination_freq_by_round[round_num].get(combo, 0) for round_num in rounds]
            ax2.plot(rounds, freqs, marker='s', label=f'{combo}')
        
        ax2.set_title('Top 5 Symbol Combinations Over Time', fontsize=14, fontweight='bold')
        ax2.set_xlabel('Round Number')
        ax2.set_ylabel('Frequency')
        ax2.legend(title='Combinations')
        ax2.grid(True, alpha=0.3)
        
        # 3. æ–°è¯é¢‘ç‡è¶‹åŠ¿
        ax3 = axes[2]
        for symbol in top_new_symbols:
            freqs = [new_symbol_freq_by_round[round_num].get(symbol, 0) for round_num in rounds]
            ax3.plot(rounds, freqs, marker='^', label=f'{symbol}')
        
        ax3.set_title('New Symbols Frequency Over Time', fontsize=14, fontweight='bold')
        ax3.set_xlabel('Round Number')
        ax3.set_ylabel('Frequency')
        ax3.legend(title='New Symbols')
        ax3.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾åƒ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = output_dir / f"frequency_trends_{timestamp}.png"
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"\nğŸ“Š å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜è‡³: {output_path}")
        
        # ä¿å­˜æ•°æ®ç”¨äºåç»­åˆ†æ
        data_output = {
            'rounds': rounds,
            'top_symbols': top_symbols,
            'top_combinations': top_combinations,
            'top_new_symbols': top_new_symbols,
            'symbol_freq_by_round': {str(k): dict(v) for k, v in symbol_freq_by_round.items()},
            'combination_freq_by_round': {str(k): dict(v) for k, v in combination_freq_by_round.items()},
            'new_symbol_freq_by_round': {str(k): dict(v) for k, v in new_symbol_freq_by_round.items()}
        }
        
        data_file = output_dir / f"frequency_data_{timestamp}.json"
        with open(data_file, 'w', encoding='utf-8') as f:
            json.dump(data_output, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“ æ•°æ®æ–‡ä»¶å·²ä¿å­˜è‡³: {data_file}")
        
        return data_output
    
    def analyze(self):
        """æ‰§è¡Œå®Œæ•´åˆ†æ"""
        output_lines = []
        def log_func(s=""):
            print(s)
            output_lines.append(str(s))
            
        log_func("=" * 70)
        log_func("è¯­è¨€æ¶Œç°åˆ†ææŠ¥å‘Š")
        log_func("=" * 70)
        
        total_rounds = len(self.data)
        total_conversations = sum(len(r.get('conversations', [])) for r in self.data)
        
        log_func(f"\nğŸ“Š åŸºç¡€ç»Ÿè®¡")
        log_func(f"  æ€»è½®æ¬¡: {total_rounds}")
        log_func(f"  æ€»å¯¹è¯æ•° (messages): {total_conversations}")
        
        # åˆ†ææ¯è½®å¯¹è¯
        for round_idx, round_data in enumerate(self.data):
            conversations = round_data.get('conversations', [])
            for conv in conversations:
                speaker = conv.get('speaker', '')
                novlang_content = conv.get('novlang', '')
                
                # æå–ç¬¦å·
                symbols = self.extract_symbols(novlang_content)
                
                # æå–ç»„åˆ
                combinations = self.extract_combinations(novlang_content)
                for combo in combinations:
                    self.symbol_combinations[combo] += 1
                
                # æ›´æ–°è¯´è¯è€…ç»Ÿè®¡
                self.speaker_stats[speaker]['total_messages'] += 1
                for symbol in symbols:
                    # åˆ¤æ–­æ˜¯å¦ä¸ºæ–°å¢ç¬¦å·
                    if symbol in NEW_SYMBOLS or symbol in NEW_OPERATORS:
                        self.speaker_stats[speaker]['new_symbol_usage'][symbol] += 1
                    else:
                        self.speaker_stats[speaker]['symbol_usage'][symbol] += 1
        
        # è®¡ç®—å¹³å‡å€¼
        for speaker, stats in self.speaker_stats.items():
            if stats['total_messages'] > 0:
                total_symbols = sum(stats['symbol_usage'].values()) + sum(stats['new_symbol_usage'].values())
                stats['avg_symbols_per_message'] = total_symbols / stats['total_messages']
                
                # è®¡ç®—æ–°ç¬¦å·é‡‡ç”¨ç‡
                total_symbol_usage = sum(stats['symbol_usage'].values()) + sum(stats['new_symbol_usage'].values())
                if total_symbol_usage > 0:
                    stats['new_symbol_adoption_rate'] = sum(stats['new_symbol_usage'].values()) / total_symbol_usage
        
        self._print_symbol_usage(log_func)
        self._print_new_symbol_usage(log_func)  # æ–°å¢ï¼šå•ç‹¬æ‰“å°æ–°ç¬¦å·ç»Ÿè®¡
        self._print_operator_usage(log_func)
        self._print_combinations(log_func)
        
        # Add N-gram analysis
        self.analyze_ngrams(n=2, top_k=10, log_func=log_func)
        self.analyze_ngrams(n=3, top_k=10, log_func=log_func)
        
        self._print_speaker_stats(log_func)
        self._print_emergence_indicators(log_func)
        self._print_new_symbol_analysis(log_func)  # æ–°å¢ï¼šæ–°ç¬¦å·ä¸“é¢˜åˆ†æ

        # Save report
        report_file = self.rounds_file.parent / "analysis_report.txt"
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(output_lines))
            print(f"\nâœ… å®Œæ•´æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_file}")
        except Exception as e:
            print(f"\nâŒ ä¿å­˜å¤±è´¥: {e}")
        
        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        try:
            self.visualize_frequency_trends()
        except Exception as e:
            print(f"\nâš  å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
    
    def _print_symbol_usage(self, log_func=print):
        """æ‰“å°åŸºç¡€ç¬¦å·ä½¿ç”¨é¢‘ç‡"""
        if self.symbol_usage:
            log_func(f"\nğŸ”¤ åŸºç¡€ç¬¦å·ä½¿ç”¨é¢‘ç‡ (Top 10)")
            log_func("-" * 70)
            for symbol, count in self.symbol_usage.most_common(10):
                meaning = NOVLANG_SYMBOLS.get(symbol, 'æœªçŸ¥')
                total_usage = sum(self.symbol_usage.values()) + sum(self.new_symbol_usage.values())
                percentage = (count / total_usage) * 100 if total_usage > 0 else 0
                log_func(f"  {symbol}  {meaning:15s}  ä½¿ç”¨æ¬¡æ•°: {count:4d}  å æ¯”: {percentage:5.2f}%")
    
    def _print_new_symbol_usage(self, log_func=print):
        """æ‰“å°æ–°å¢ç¬¦å·ä½¿ç”¨é¢‘ç‡"""
        if self.new_symbol_usage or self.new_operator_usage:
            log_func(f"\nğŸ†• æ–°å¢ç¬¦å·ä½¿ç”¨ç»Ÿè®¡")
            log_func("-" * 70)
            
            # ç»Ÿè®¡æ–°å¢ç¬¦å·
            if self.new_symbol_usage:
                log_func("  æ–°å¢åŸºç¡€ç¬¦å·:")
                total_new_symbols = sum(self.new_symbol_usage.values())
                total_all_symbols = sum(self.symbol_usage.values()) + total_new_symbols
                
                for symbol, count in self.new_symbol_usage.most_common():
                    meaning = NEW_SYMBOLS.get(symbol, 'æœªçŸ¥')
                    percentage_all = (count / total_all_symbols) * 100 if total_all_symbols > 0 else 0
                    percentage_new = (count / total_new_symbols) * 100 if total_new_symbols > 0 else 0
                    log_func(f"    {symbol}  {meaning:15s}  ä½¿ç”¨æ¬¡æ•°: {count:4d}  æ€»ç¬¦å·å æ¯”: {percentage_all:5.2f}%  æ–°ç¬¦å·å æ¯”: {percentage_new:5.2f}%")
            
            # ç»Ÿè®¡æ–°å¢æ“ä½œç¬¦
            if self.new_operator_usage:
                log_func("\n  æ–°å¢æ“ä½œç¬¦:")
                total_new_operators = sum(self.new_operator_usage.values())
                total_all_operators = sum(self.operator_usage.values()) + total_new_operators
                
                for operator, count in self.new_operator_usage.most_common():
                    meaning = NEW_OPERATORS.get(operator, 'æœªçŸ¥')
                    percentage_all = (count / total_all_operators) * 100 if total_all_operators > 0 else 0
                    log_func(f"    {operator}  {meaning:20s}  ä½¿ç”¨æ¬¡æ•°: {count:4d}  æ€»æ“ä½œç¬¦å æ¯”: {percentage_all:5.2f}%")
    
    def _print_operator_usage(self, log_func=print):
        """æ‰“å°æ“ä½œç¬¦ä½¿ç”¨é¢‘ç‡"""
        if self.operator_usage:
            log_func(f"\nğŸ”§ åŸºç¡€ç»„åˆæ ‡è®°ä½¿ç”¨é¢‘ç‡")
            log_func("-" * 70)
            total_operators = sum(self.operator_usage.values()) + sum(self.new_operator_usage.values())
            for operator, count in self.operator_usage.most_common():
                meaning = NOVLANG_OPERATORS.get(operator, 'æœªçŸ¥')
                percentage = (count / total_operators) * 100 if total_operators > 0 else 0
                log_func(f"  {operator}  {meaning:20s}  ä½¿ç”¨æ¬¡æ•°: {count:4d}  å æ¯”: {percentage:5.2f}%")
    
    def _print_combinations(self, log_func=print):
        """æ‰“å°å¸¸è§ç¬¦å·ç»„åˆ"""
        if self.symbol_combinations.most_common(15):
            for combo, count in self.symbol_combinations.most_common(15):
                if count >= 2:  # åªæ˜¾ç¤ºå‡ºç°2æ¬¡ä»¥ä¸Šçš„ç»„åˆ
                    # æ ‡è®°åŒ…å«æ–°ç¬¦å·çš„ç»„åˆ
                    has_new_symbol = any(symbol in combo for symbol in NEW_SYMBOLS.keys()) or any(operator in combo for operator in NEW_OPERATORS.keys())
                    new_marker = "[æ–°]" if has_new_symbol else "    "
                    log_func(f"  {combo:10s} {new_marker} å‡ºç°æ¬¡æ•°: {count:3d}")
        else:
            log_func("  æš‚æ— è¶³å¤Ÿæ•°æ®å‘ç°é«˜é¢‘ç»„åˆ")

    def analyze_ngrams(self, n=2, top_k=10, log_func=print):
        """åˆ†æN-gram (Nå…ƒè¯­æ³•) æ¶Œç°æƒ…å†µ"""
        from collections import Counter, defaultdict
        
        # Valid symbols set for filtering
        valid_chars = set()
        valid_chars.update(NOVLANG_SYMBOLS.keys())
        valid_chars.update(NOVLANG_OPERATORS.keys())
        valid_chars.update(NEW_SYMBOLS.keys())
        valid_chars.update(NEW_OPERATORS.keys())
        valid_chars.update(PRAGMATIC_SYMBOLS.keys())

        # 1. æ”¶é›†æ‰€æœ‰è½®æ¬¡çš„ Novlang Token
        # tokens_per_round = { round_num: [tokens...] }
        tokens_per_round = defaultdict(list)
        all_ngrams = Counter()
        ngrams_per_round = defaultdict(Counter)

        for round_data in self.data:
            round_num = round_data.get('round', 0)
            
            # æå–æœ¬è½®æ‰€æœ‰ token
            round_tokens = []
            for conv in round_data.get('conversations', []):
                novlang = conv.get('novlang', '')
                if not novlang: continue
                
                # ç®€å•åˆ†è¯: æš‚æ—¶æŒ‰å­—ç¬¦æˆ–è€…ç‰¹å®šåˆ†éš”ç¬¦åˆ†è¯
                # å‡è®¾ Novlang ä»¥ Â· æˆ–ç©ºæ ¼åˆ†éš”ï¼Œæˆ–è€…ç›´æ¥åˆ†æå­—ç¬¦æµ
                # è¿™é‡Œæˆ‘ä»¬é‡‡ç”¨ä¸€ç§æ··åˆç­–ç•¥ï¼šä¿ç•™å®Œæ•´ç¬¦å·ï¼Œå¿½ç•¥çº¯æ ‡ç‚¹ï¼ˆé™¤äº†Novlangæ“ä½œç¬¦ï¼‰
                
                # æ–¹æ³• A: å­—ç¬¦çº§ N-gram (é€‚åˆç´§å‡‘çš„ Novlang)
                # è¿‡æ»¤æ‰éç¬¦å·å­—ç¬¦ (è¿™é‡Œå‡è®¾æ‰€æœ‰æ„Ÿå…´è¶£çš„éƒ½åœ¨å®šä¹‰çš„å­—å…¸é‡Œï¼Œæˆ–è€…æ˜¯æ±‰å­—/å•è¯)
                # ä½†æ··åˆäº†ä¸­æ–‡è¯æ±‡çš„ Novlang æ¯”è¾ƒå¤æ‚ï¼Œæˆ‘ä»¬å…ˆè¯•ç€æŒ‰ 'Â·' åˆ†å‰²
                
                # å°è¯•åˆ†å‰²:
                # 1. æ›¿æ¢æ‰ä¸€äº›å¹²æ‰°å­—ç¬¦
                clean_text = novlang.replace('\n', ' ').strip()
                # 2. æŒ‰ 'Â·' åˆ†å‰²æˆ "è¯" (Token)
                raw_tokens = [t.strip() for t in clean_text.split('Â·') if t.strip()]

                # 3. è¿‡æ»¤æ±‰å­—å’Œæ— å…³å­—ç¬¦ï¼Œåªä¿ç•™æœ‰æ•ˆç¬¦å·
                filtered_tokens = []
                for t in raw_tokens:
                    # Keep only chars in valid_chars
                    cleaned = "".join([c for c in t if c in valid_chars])
                    if cleaned:
                        filtered_tokens.append(cleaned)
                
                round_tokens.extend(filtered_tokens)

            tokens_per_round[round_num] = round_tokens

            # ç”Ÿæˆ N-grams
            if len(round_tokens) >= n:
                grams = [tuple(round_tokens[i:i+n]) for i in range(len(round_tokens)-n+1)]
                ngrams_per_round[round_num].update(grams)
                all_ngrams.update(grams)
        
        log_func("\n" + "="*70)
        log_func(f"ğŸ§® {n}-gram (Nå…ƒç»„) æ¶Œç°åˆ†æ")
        log_func("="*70)

        if not all_ngrams:
            log_func("  æš‚æ— è¶³å¤Ÿæ•°æ®è¿›è¡Œ N-gram åˆ†æ")
            return

        # 2. æ‰¾å‡ºé«˜é¢‘ N-gram
        top_grams = all_ngrams.most_common(top_k)
        
        log_func(f"\nğŸ† å…¨å±€ Top {top_k} {n}-grams:")
        for gram, count in top_grams:
            log_func(f"  {' Â· '.join(gram)} (Count: {count})")

        # 3. è®¡ç®—æ¶Œç°æŒ‡æ ‡ (ç®€å•ç‰ˆ: çªå¢æ£€æµ‹)
        # å®šä¹‰: åœ¨å‰ X è½®å¾ˆå°‘å‡ºç°ï¼Œä½†åœ¨å Y è½®é¢‘ç¹å‡ºç°
        log_func(f"\nğŸš€ æ½œåœ¨çš„æ¶Œç°æ­é… (Emerging Collocations):")
        log_func(f"  (ç­›é€‰æ ‡å‡†: å‰åŠç¨‹å‡ºç°ç‡ < 20% ä¸” ååŠç¨‹å‡ºç°æ¬¡æ•° >= 3)")
        
        sorted_rounds = sorted(tokens_per_round.keys())
        if len(sorted_rounds) < 2:
            log_func("  è½®æ¬¡è¿‡å°‘ï¼Œæ— æ³•è®¡ç®—æ¶Œç°è¶‹åŠ¿")
            return

        mid_point = len(sorted_rounds) // 2
        early_rounds = sorted_rounds[:mid_point]
        late_rounds = sorted_rounds[mid_point:]
        
        # éå†æ‰€æœ‰å‡ºç°è¿‡è‡³å°‘ 3 æ¬¡çš„ ngram
        potential_emergence = []
        for gram, total_count in all_ngrams.items():
            if total_count < 3: continue
            
            count_early = sum(ngrams_per_round[r][gram] for r in early_rounds)
            count_late = sum(ngrams_per_round[r][gram] for r in late_rounds)
            
            # ç®€å•æŒ‡æ ‡: åæœŸå æ¯”
            late_ratio = count_late / total_count
            
            if late_ratio > 0.8 and count_early <= 1: # 80% ä»¥ä¸Šå‡ºç°åœ¨ååŠç¨‹ï¼Œä¸”å‰åŠç¨‹å‡ ä¹æ²¡æœ‰
                potential_emergence.append({
                    'gram': gram,
                    'count': total_count,
                    'early': count_early,
                    'late': count_late
                })
        
        # æŒ‰æ€»æ¬¡æ•°æ’åº
        potential_emergence.sort(key=lambda x: x['count'], reverse=True)
        
        if potential_emergence:
            for item in potential_emergence:
                gram_str = ' Â· '.join(item['gram'])
                log_func(f"  ğŸ”¥ {gram_str}")
                log_func(f"     Total: {item['count']} | Early Rounds: {item['early']} | Late Rounds: {item['late']}")
        else:
            log_func("  æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„æ¶Œç°æ¨¡å¼ (æ ¹æ®å½“å‰é˜ˆå€¼)")

    def _print_speaker_stats(self, log_func=print):
        """æ‰“å°å„è¯´è¯è€…çš„ç»Ÿè®¡"""
        log_func(f"\nğŸ‘¥ å„è§’è‰²ç»Ÿè®¡")
        log_func("-" * 70)
        for speaker, stats in sorted(self.speaker_stats.items()):
            log_func(f"\n  {speaker}")
            log_func(f"    æ€»å‘è¨€æ•°: {stats['total_messages']}")
            log_func(f"    å¹³å‡ç¬¦å·æ•°/æ¶ˆæ¯: {stats['avg_symbols_per_message']:.2f}")
            log_func(f"    æ–°ç¬¦å·é‡‡ç”¨ç‡: {stats['new_symbol_adoption_rate']:.2%}")
            
            # æœ€å¸¸ç”¨åŸºç¡€ç¬¦å·
            if stats['symbol_usage']:
                log_func(f"    æœ€å¸¸ç”¨åŸºç¡€ç¬¦å·:")
                for symbol, count in stats['symbol_usage'].most_common(3):
                    meaning = NOVLANG_SYMBOLS.get(symbol, 'æœªçŸ¥')
                    log_func(f"      {symbol} ({meaning}): {count}æ¬¡")
            
            # æœ€å¸¸ç”¨æ–°å¢ç¬¦å·
            if stats['new_symbol_usage']:
                log_func(f"    æœ€å¸¸ç”¨æ–°å¢ç¬¦å·:")
                for symbol, count in stats['new_symbol_usage'].most_common(3):
                    meaning = NEW_SYMBOLS.get(symbol, NEW_OPERATORS.get(symbol, 'æœªçŸ¥'))
                    log_func(f"      {symbol} ({meaning}): {count}æ¬¡")
    
    def _print_new_symbol_analysis(self, log_func=print):
        """æ–°å¢ï¼šä¸“é¢˜åˆ†ææ–°ç¬¦å·ä½¿ç”¨æƒ…å†µ"""
        log_func(f"\nğŸ“ˆ æ–°å¢ç¬¦å·ä¸“é¢˜åˆ†æ")
        log_func("-" * 70)
        
        # 1. æ–°ç¬¦å·æ€»ä½“é‡‡ç”¨æƒ…å†µ
        total_old_symbols = sum(self.symbol_usage.values())
        total_new_symbols = sum(self.new_symbol_usage.values())
        total_all_symbols = total_old_symbols + total_new_symbols
        
        if total_all_symbols > 0:
            new_symbol_percentage = (total_new_symbols / total_all_symbols) * 100
            log_func(f"  æ–°ç¬¦å·æ€»ä½“é‡‡ç”¨ç‡: {new_symbol_percentage:.2f}% ({total_new_symbols}/{total_all_symbols})")
        
        # 2. æ–°ç¬¦å·åœ¨å„è½®æ¬¡ä¸­çš„ä½¿ç”¨è¶‹åŠ¿
        log_func(f"\n  æ–°ç¬¦å·ä½¿ç”¨æ—¶é—´çº¿:")
        new_symbols_by_round = []
        for round_idx, round_data in enumerate(self.data):
            round_new_symbols = 0
            round_all_symbols = 0
            
            for conv in round_data.get('conversations', []):
                novlang_content = conv.get('novlang', '')
                # ç»Ÿè®¡æœ¬è½®æ‰€æœ‰ç¬¦å·
                for symbol in NOVLANG_SYMBOLS.keys():
                    round_all_symbols += novlang_content.count(symbol)
                for symbol in NEW_SYMBOLS.keys():
                    count = novlang_content.count(symbol)
                    round_all_symbols += count
                    round_new_symbols += count
            
            if round_all_symbols > 0:
                percentage = (round_new_symbols / round_all_symbols) * 100
                new_symbols_by_round.append(percentage)
                log_func(f"    ç¬¬{round_idx+1}è½®: {percentage:.1f}% ({round_new_symbols}/{round_all_symbols})")
        
        # 3. æ–°ç¬¦å·çš„ç»„åˆèƒ½åŠ›
        log_func(f"\n  æ–°ç¬¦å·ç»„åˆèƒ½åŠ›:")
        new_symbol_combinations = []
        for combo, count in self.symbol_combinations.items():
            # æ£€æŸ¥ç»„åˆæ˜¯å¦åŒ…å«æ–°ç¬¦å·
            has_new_symbol = any(symbol in combo for symbol in NEW_SYMBOLS.keys()) or any(operator in combo for operator in NEW_OPERATORS.keys())
            if has_new_symbol and count >= 2:
                new_symbol_combinations.append((combo, count))
        
        if new_symbol_combinations:
            new_symbol_combinations.sort(key=lambda x: x[1], reverse=True)
            log_func(f"    åŒ…å«æ–°ç¬¦å·çš„ç¨³å®šç»„åˆ ({len(new_symbol_combinations)}ä¸ª):")
            for combo, count in new_symbol_combinations[:5]:
                log_func(f"      {combo}: {count}æ¬¡")
        else:
            log_func(f"    æš‚æ— åŒ…å«æ–°ç¬¦å·çš„ç¨³å®šç»„åˆ")
    
    def _print_emergence_indicators(self, log_func=print):
        """æ‰“å°è¯­è¨€æ¶Œç°æŒ‡æ ‡"""
        log_func(f"\nâœ¨ è¯­è¨€æ¶Œç°æŒ‡æ ‡")
        log_func("-" * 70)
        
        # 1. ç¬¦å·å¤šæ ·æ€§ï¼ˆåŒ…æ‹¬æ–°æ—§ç¬¦å·ï¼‰
        all_symbols_dict = {**NOVLANG_SYMBOLS, **NEW_SYMBOLS}
        total_unique_symbols = len(self.symbol_usage) + len(self.new_symbol_usage)
        total_possible_symbols = len(all_symbols_dict)
        symbol_diversity = total_unique_symbols / total_possible_symbols
        log_func(f"  ç¬¦å·å¤šæ ·æ€§: {symbol_diversity:.2%} ({total_unique_symbols}/{total_possible_symbols})")
        
        # 2. æ–°ç¬¦å·å¤šæ ·æ€§
        total_unique_new_symbols = len(self.new_symbol_usage)
        total_possible_new_symbols = len(NEW_SYMBOLS)
        new_symbol_diversity = total_unique_new_symbols / total_possible_new_symbols if total_possible_new_symbols > 0 else 0
        log_func(f"  æ–°ç¬¦å·å¤šæ ·æ€§: {new_symbol_diversity:.2%} ({total_unique_new_symbols}/{total_possible_new_symbols})")
        
        # 3. ç»„åˆåˆ›æ–°åº¦
        unique_combinations = len(self.symbol_combinations)
        log_func(f"  ç‹¬ç‰¹ç»„åˆæ•°: {unique_combinations}")
        
        # 4. ç¨³å®šç»„åˆï¼ˆå‡ºç°5æ¬¡ä»¥ä¸Šï¼‰
        stable_combos = [combo for combo, count in self.symbol_combinations.items() if count >= 5]
        log_func(f"  ç¨³å®šç»„åˆæ•°: {len(stable_combos)} (å‡ºç°â‰¥5æ¬¡)")
        if stable_combos:
            # æ ‡è®°åŒ…å«æ–°ç¬¦å·çš„ç¨³å®šç»„åˆ
            stable_with_new = [combo for combo in stable_combos if 
                               any(symbol in combo for symbol in NEW_SYMBOLS.keys()) or 
                               any(operator in combo for operator in NEW_OPERATORS.keys())]
            log_func(f"    å…¶ä¸­åŒ…å«æ–°ç¬¦å·: {len(stable_with_new)}ä¸ª")
            if stable_combos:
                log_func(f"    {', '.join(stable_combos[:10])}")
        
        # 5. ä½¿ç”¨é›†ä¸­åº¦ï¼ˆåŸºå°¼ç³»æ•°çš„ç®€åŒ–ç‰ˆï¼‰
        all_usage = dict(self.symbol_usage)
        all_usage.update(self.new_symbol_usage)
        if all_usage:
            total_usage = sum(all_usage.values())
            top_5_usage = sum(count for _, count in Counter(all_usage).most_common(5))
            concentration = top_5_usage / total_usage
            log_func(f"  ç¬¦å·é›†ä¸­åº¦: {concentration:.2%} (Top5ç¬¦å·å æ¯”)")
        
        # 6. æ¼”åŒ–è¶‹åŠ¿ï¼ˆæ–°ç¬¦å·çš„å‡ºç°æƒ…å†µï¼‰
        if len(self.data) >= 4:  # è‡³å°‘æœ‰4è½®æ•°æ®
            early_rounds = self.data[:len(self.data)//2]
            late_rounds = self.data[len(self.data)//2:]
            
            early_symbols = set()
            late_symbols = set()
            
            for round_data in early_rounds:
                for conv in round_data.get('conversations', []):
                    novlang = conv.get('novlang', '')
                    # æå–æ‰€æœ‰ç¬¦å·ï¼ˆåŒ…æ‹¬æ–°ç¬¦å·ï¼‰
                    for symbol in all_symbols_dict.keys():
                        if symbol in novlang:
                            early_symbols.add(symbol)
            
            for round_data in late_rounds:
                for conv in round_data.get('conversations', []):
                    novlang = conv.get('novlang', '')
                    for symbol in all_symbols_dict.keys():
                        if symbol in novlang:
                            late_symbols.add(symbol)
            
            new_symbols = late_symbols - early_symbols
            lost_symbols = early_symbols - late_symbols
            
            # åªå…³æ³¨æ–°å¢ç¬¦å·ä¸­çš„æ–°å‡ºç°
            new_symbols_in_new_set = [s for s in new_symbols if s in NEW_SYMBOLS or s in NEW_OPERATORS]
            
            log_func(f"  æ–°å‡ºç°ç¬¦å·æ•°: {len(new_symbols)} (ååŠæœŸ)")
            log_func(f"  å…¶ä¸­æ–°å¢ç¬¦å·: {len(new_symbols_in_new_set)}ä¸ª")
            if new_symbols_in_new_set:
                log_func(f"    {', '.join(sorted(new_symbols_in_new_set))}")
            
            if lost_symbols:
                log_func(f"  æ¶ˆå¤±ç¬¦å·æ•°: {len(lost_symbols)} (å‰åŠæœŸæœ‰ï¼ŒååŠæœŸæ— )")
                if len(lost_symbols) <= 10:
                    log_func(f"    {', '.join(sorted(lost_symbols))}")
    
    def export_timeline(self, output_file):
        """å¯¼å‡ºæ—¶é—´çº¿æ•°æ®ï¼ˆå¯ç”¨äºå¯è§†åŒ–ï¼‰"""
        timeline = []
        for round_data in self.data:
            round_num = round_data.get('round', 0)
            round_symbols = Counter()
            round_new_symbols = Counter()
            
            for conv in round_data.get('conversations', []):
                novlang_content = conv.get('novlang', '')
                
                # ç»Ÿè®¡åŸºç¡€ç¬¦å·
                for symbol in NOVLANG_SYMBOLS.keys():
                    count = novlang_content.count(symbol)
                    if count > 0:
                        round_symbols[symbol] += count
                
                # ç»Ÿè®¡æ–°å¢ç¬¦å·
                for symbol in NEW_SYMBOLS.keys():
                    count = novlang_content.count(symbol)
                    if count > 0:
                        round_new_symbols[symbol] += count
            
            timeline.append({
                'round': round_num,
                'total_symbols': sum(round_symbols.values()) + sum(round_new_symbols.values()),
                'total_new_symbols': sum(round_new_symbols.values()),
                'unique_symbols': len(round_symbols) + len(round_new_symbols),
                'unique_new_symbols': len(round_new_symbols),
                'new_symbol_ratio': sum(round_new_symbols.values()) / (sum(round_symbols.values()) + sum(round_new_symbols.values())) if (sum(round_symbols.values()) + sum(round_new_symbols.values())) > 0 else 0,
                'top_symbols': dict(round_symbols.most_common(3)),
                'top_new_symbols': dict(round_new_symbols.most_common(3))
            })
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(timeline, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ“ æ—¶é—´çº¿æ•°æ®å·²å¯¼å‡ºåˆ°: {output_file}")


def main():
    parser = argparse.ArgumentParser(description='åˆ†æè¯­è¨€æ¶Œç°å®éªŒæ•°æ®')
    parser.add_argument('rounds_file', help='rounds.jsonæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--export-timeline', help='å¯¼å‡ºæ—¶é—´çº¿æ•°æ®åˆ°æŒ‡å®šæ–‡ä»¶')
    parser.add_argument('--trace', help='è¿½è¸ªç‰¹å®šè¯æ±‡çš„æ¶Œç°è¿‡ç¨‹ï¼ˆæŒ‰æ—¶é—´é¡ºåºè¾“å‡ºï¼‰')
    parser.add_argument('--visualize', action='store_true', help='ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨')
    args = parser.parse_args()
    
    analyzer = LanguageEmergenceAnalyzer(args.rounds_file)
    
    if args.trace:
        # è‡ªåŠ¨ç”Ÿæˆè¿½è¸ªæŠ¥å‘Šæ–‡ä»¶å
        trace_file = Path(args.rounds_file).parent / f"trace_{args.trace}.txt"
        analyzer.trace_emergence(args.trace, trace_file)
    elif args.visualize:
        # åªç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        analyzer.visualize_frequency_trends()
    else:
        analyzer.analyze()
    
    if args.export_timeline:
        analyzer.export_timeline(args.export_timeline)


if __name__ == '__main__':
    main()